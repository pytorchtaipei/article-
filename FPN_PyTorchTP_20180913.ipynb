{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FPN_PyTorchTP_20180913.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "5DfZBkkbILB9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature Pyramid Network (FPN) in PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "hPuDoo-NHWFK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook is the tutorial of [Feature Pyramid Network](https://arxiv.org/abs/1612.03144) (applied on ResNet) for [PyTorch Taipei](https://pytorchtaipei.github.io/).\n",
        "* The code is modified from [kuangliu's repo](https://github.com/kuangliu/pytorch-fpn/blob/master/fpn.py).\n",
        "* Only model construction  in PyTorch (no training/testing in this notebook!).\n",
        "* Other FPN repo: [yangxue0827(tf)](https://github.com/yangxue0827/FPN_Tensorflow), [unsky(caffe)](https://github.com/unsky/FPN), [xmyqsh](https://github.com/xmyqsh/FPN)"
      ]
    },
    {
      "metadata": {
        "id": "FYKLxI2JIe6d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Enabling PyTorch in Colab\n",
        "more info of Colab: https://mattwang44.github.io/en/articles/colab/"
      ]
    },
    {
      "metadata": {
        "id": "ZbZrHcTW43u2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2uyfaFyX63aa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# torch packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OePbo2Ye7loS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Residual Blocks in ResNet"
      ]
    },
    {
      "metadata": {
        "id": "xxG-wnuG8Ar_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ref:\n",
        "1. Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "2. [Understand Deep Residual Networks](https://blog.waya.ai/deep-residual-learning-9610bb62c355)"
      ]
    },
    {
      "metadata": {
        "id": "sKz1RZ917J-_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![bottleneck](https://cdn-images-1.medium.com/max/1600/1*HYrB7apC0lbXDTzSDs2OFg.png)"
      ]
    },
    {
      "metadata": {
        "id": "ea37sN8466KT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# use the \"bottleneck\" residual block (right graph above)\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channel, channel, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channel, channel, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channel)\n",
        "        self.conv2 = nn.Conv2d(channel, channel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channel)\n",
        "        self.conv3 = nn.Conv2d(channel, self.expansion*channel, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*channel)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        \n",
        "        # assuring two images tend to be added up (element-wise) has same size\n",
        "        if stride != 1 or in_channel != self.expansion*channel:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channel, self.expansion*channel, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*channel)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2hnApeVr8kP3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ResNet-FPN model"
      ]
    },
    {
      "metadata": {
        "id": "57020rpoEkrB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ref:\n",
        "[Understanding Feature Pyramid Networks ](https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c) (This is a fabulous blog post!!)"
      ]
    },
    {
      "metadata": {
        "id": "wXhhvoePEdlp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![ResNet-FPN model](https://cdn-images-1.medium.com/max/1600/1*cHR4YRqdPBOx4IDqzU-GwQ.png =750x450)"
      ]
    },
    {
      "metadata": {
        "id": "QPGapx0346Zs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FPN_Res(nn.Module):\n",
        "    def __init__(self, block, num_blocks):\n",
        "        \"\"\"\n",
        "        blocks: bottleneck object\n",
        "        num_blocks: a list of number, each refers to the number of bottleneck unit in a \"stage\".\n",
        "        \"\"\"\n",
        "        super(FPN_Res, self).__init__()\n",
        "        self.in_channel = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Bottom-up layers\n",
        "        self.layer1 = self._make_stage(block,  64, num_blocks[0], stride=1)  #C2\n",
        "        self.layer2 = self._make_stage(block, 128, num_blocks[1], stride=2)  #C3\n",
        "        self.layer3 = self._make_stage(block, 256, num_blocks[2], stride=2)  #C4\n",
        "        self.layer4 = self._make_stage(block, 512, num_blocks[3], stride=2)  #C5\n",
        "\n",
        "        # Top layer (M5, for reducing channels)\n",
        "        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)  \n",
        "\n",
        "        # Lateral layers\n",
        "        self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n",
        "        self.latlayer2 = nn.Conv2d( 512, 256, kernel_size=1, stride=1, padding=0)\n",
        "        self.latlayer3 = nn.Conv2d( 256, 256, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        # Smooth layers (each produce a \"head\")\n",
        "        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth4 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "    def _make_stage(self, block, channel, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)                                   #use assigned stride for the 1st res-block, stride=1 for the rest \n",
        "        layers = []\n",
        "        for stride in strides:                                                    #create list of res-blocks\n",
        "            layers.append(block(self.in_channel, channel, stride))\n",
        "            self.in_channel = channel * block.expansion\n",
        "        return nn.Sequential(*layers)                                             #transfer a list of res-blocks to a stage \n",
        "      \n",
        "    def _upsample_add(self, x, y):\n",
        "        # Please see https://pytorch.org/docs/stable/nn.html#torch.nn.functional.upsample\n",
        "        '''\n",
        "        Upsample and add two feature maps.\n",
        "        Args:\n",
        "          x: (Variable) top feature map to be upsampled.\n",
        "          y: (Variable) lateral feature map.\n",
        "        Returns:\n",
        "          (Variable) added feature map.\n",
        "        Note in PyTorch, when input size is odd, the upsampled feature map\n",
        "        with `F.upsample(..., scale_factor=2, mode='nearest')`\n",
        "        maybe not equal to the lateral feature map size.\n",
        "        e.g.\n",
        "        original input size: [N,_,15,15] ->\n",
        "        conv2d feature map size: [N,_,8,8] ->\n",
        "        upsampled feature map size: [N,_,16,16]\n",
        "        So we choose bilinear upsample which supports arbitrary output sizes.\n",
        "        '''\n",
        "        _,_,H,W = y.size()\n",
        "        return F.upsample(x, size=(H,W), mode='bilinear') + y\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Bottom-up\n",
        "        C1 = F.relu(self.bn1(self.conv1(x)))\n",
        "        C1 = F.max_pool2d(C1, kernel_size=3, stride=2, padding=1)\n",
        "        C2 = self.layer1(C1)\n",
        "        C3 = self.layer2(C2)\n",
        "        C4 = self.layer3(C3)\n",
        "        C5 = self.layer4(C4)\n",
        "        # Top-down\n",
        "        M5 = self.toplayer(C5)\n",
        "        M4 = self._upsample_add(M5, self.latlayer1(C4))\n",
        "        M3 = self._upsample_add(M4, self.latlayer2(C3))\n",
        "        M2 = self._upsample_add(M3, self.latlayer3(C2))\n",
        "        # Smooth\n",
        "        P5 = self.smooth1(M5)\n",
        "        P4 = self.smooth2(M4)\n",
        "        P3 = self.smooth3(M3)\n",
        "        P2 = self.smooth4(M2)\n",
        "        return P2, P3, P4, P5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ccz9vX_AN34J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Different ResNet with different number of layers:\n",
        "![res](https://images2017.cnblogs.com/blog/606386/201710/606386-20171016223757443-785220142.png =720x300)"
      ]
    },
    {
      "metadata": {
        "id": "W48QwrB8NE54",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def FPN_Res18():\n",
        "    return FPN_Res(Bottleneck, [2,2,2,2]) # use bottleneck instead of naive res-block\n",
        "def FPN_Res50():\n",
        "    return FPN_Res(Bottleneck, [3,4,6,3])\n",
        "def FPN_Res101():\n",
        "    return FPN_Res(Bottleneck, [3,4,23,3])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_mWOLqfDWaLu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check (print out the size of heads)"
      ]
    },
    {
      "metadata": {
        "id": "E4qy55grNgwi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(net):\n",
        "    #fms = net(Variable(torch.randn(1,3,600,900)))\n",
        "    fms = net(Variable(torch.randn(1,3,64,64)))\n",
        "    for fm in fms:\n",
        "        print(fm.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9vPXNwOUR86z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "5664c150-74ba-4d8b-bb6c-c55c5941680a"
      },
      "cell_type": "code",
      "source": [
        "test(FPN_Res18())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256, 16, 16])\n",
            "torch.Size([1, 256, 8, 8])\n",
            "torch.Size([1, 256, 4, 4])\n",
            "torch.Size([1, 256, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ch514xBW29F7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "bdf7f868-9753-4111-aed4-3e7c9c754083"
      },
      "cell_type": "code",
      "source": [
        "test(FPN_Res50())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256, 16, 16])\n",
            "torch.Size([1, 256, 8, 8])\n",
            "torch.Size([1, 256, 4, 4])\n",
            "torch.Size([1, 256, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-kMBkZ3XR9pt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "fbdb523e-2e97-4dd5-be8a-600bb621be79"
      },
      "cell_type": "code",
      "source": [
        "test(FPN_Res101())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256, 16, 16])\n",
            "torch.Size([1, 256, 8, 8])\n",
            "torch.Size([1, 256, 4, 4])\n",
            "torch.Size([1, 256, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}